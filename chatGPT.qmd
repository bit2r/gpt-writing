# 챗GPT

## JPEG 압축

챗GPT는 웹의 무수한 데이터를 학습한 하나의 압축저장소로 볼 수 있다. 압축을 풀게되면 원본을 정확히 복원할 수 있는 경우도 있지만, 그렇지 않은 경우도 불가피하게 존재한다[@Chiang2023] .

챗GPT는 **"웹의 흐릿한 JPEG"**라는 비유로 설명된다. JPEG 기술은 손실 압축 기술로, 무손실 압축 기술인 PNG와 대조된다. 흐릿한 이미지가 선명하지 않거나 정확하지 않은 것처럼, 챗GPT 역시 항상 완벽한 답변을 제공하거나 모든 질문을 정확히 이해하는 것은 아니다. 그러나 사용자와의 상호작용을 통해 지속적인 학습을 통해 개선되고 있다. 더 많은 사람들이 챗GPT를 사용하면 할수록, 그만큼 더 정교하게 사람의 언어를 이해하고 대화할 수 있는 기술로 발전하게 된다.

챗GPT를 비롯한 유사 인공지능(AI)이 너무 강력해져 인간을 대체할 것이라는 우려도 있지만, 챗GPT는 결국 인간의 작업을 더 쉽고 효율적으로 수행할 수 있는 강력한 도구에 불과하기 때문에, 인간을 능가하거나 지배할 가능성은 거의 없다. 챗GPT 즉, 인공지능을 책임감 있고 윤리적으로 사용하는 것은 결국 가계, 기업, 정부, 우리 모두의 책임이다.

챗GPT가 간단한 사칙연산 계산에서 어려움을 겪는 것은 웹에 퍼져 있는 숫자 계산 데이터를 기반으로 학습하여 계산을 모방할 뿐이기 때문이다. 만약 사칙연산에 대한 일반적 원칙을 제대로 이해한다면, 웹에서 발견되는 사칙연산 문제뿐만 아니라 다른 계산 문제도 정확히 해결할 수 있지만 현재로는 그렇지 못하기 때문에 플러그인(Plugin)을 통해 해결하고 있다.

![JPEG와 PNG 비교](images/jpeg-vs-png-file-size.jpg){fig-align="center" width="375"}

[자료출처: [WHAT'S THE DIFFERENCE BETWEEN JPEG AND PNG: BEGINNER GUIDE](https://fixthephoto.com/tech-tips/difference-between-jpeg-and-png.html)]{.aside}

## 기초모형

기초모형(Foundation Model)이라는 새로운 패러다임은 인공지능(AI) 시스템 구축에 혁신적인 변화를 가져왔다. 이러한 모델은 광범위한 데이터에 대해 사전에 학습되어 있어, 다양한 실무하위 작업에 적용할 수 있는 범용성을 가지고 있다. BERT, GPT-3, CLIP 등이 대표적인 예로, 이들 모델은 텍스트, 이미지, 음성 등 다양한 형태의 데이터에 대한 처리가 가능하다.

기초모형의 등장은 기존의 특정 작업에 특화된 모델을 개발하는 방식을 넘어, 하나의 모델로 여러 작업을 수행할 수 있는 새로운 접근법을 제시한다. 이로 인해, 모델 개발과 학습에 드는 비용과 시간이 크게 절감되며, 더 빠르고 효율적인 AI 시스템 구축이 가능해진다.

또한, 기초모형은 미세조정(fine-tuning)을 통해 특정 작업에 적합하게 만들 수 있다. 이는 기초모형이 가진 범용성을 더욱 강화하며, 실무하위 작업에 대한 성능도 향상시킨다. 예를 들어, BERT 모델은 자연어 처리 작업에, CLIP 모델은 이미지와 텍스트를 연결하는 작업에 미세조정을 통해 효과적으로 활용될 수 있다.

이러한 기초모형의 패러다임은 AI의 미래를 크게 밝혀준다. 다양한 분야와 작업에서 활용될 수 있는 이러한 모델은 AI 기술의 적용 범위를 더욱 확장시키며, 그 가능성을 무궁무진하게 만든다. 이는 결국 AI가 사회와 산업의 다양한 분야에서 더욱 깊게 활용될 수 있음을 의미한다.

![기초 모형](images/foundation_model.png){fig-align="center" width="390"}

## 공학 발전

기계학습과 자연어 처리 분야에서는 시간이 지남에 따라 다양한 공학 기법이 등장하고 발전해왔다. 이러한 공학 기법들은 대체로 피처 공학, 아키텍처 공학, 목적 공학, 그리고 프롬프트 공학으로 구분할 수 있다. 각 기법은 특정 시기에 주목받았으며, 그에 따라 다양한 모델과 알고리즘이 개발되었다. 이러한 공학 기법들의 진화 과정을 살펴보면, 기계학습과 자연어 처리가 어떻게 발전해왔는지 이해할 수 있다.

첫 번째로, 피처 공학의 시대는 대략 2015년까지 이어졌으며, 이 기간 동안 비신경망 지도학습이 주로 사용되었다. 이 단계에서는 수작업으로 특징(Feature)을 추출하고, 그 특징을 기반으로 SVM(Support Vector Machine)이나 CRF(Conditional Random Fields) 같은 비신경망 기계학습 모델을 학습시켰다. 이 방법은 당시에는 효과적이었지만, 수작업으로 특징을 추출하는 과정이 복잡하고 시간이 많이 소요되었다.

두 번째로, 2013년부터 2018년까지는 아키텍처 공학이 주목받았다. 이 시기에는 신경망 지도학습이 대두되면서, 수작업으로 특징을 추출할 필요가 줄었다. 대신, 신경망 아키텍처를 설계하고 수정하는 작업이 중요해졌다. 예를 들어, 텍스트 분류 작업에는 CNN(Convolutional Neural Networks)이 주로 사용되었다. 이 단계에서는 아키텍처의 중요성이 강조되었으며, 사전학습된 언어 모델이나 임베딩 같은 얕은 특징도 사용되기 시작했다.

마지막으로, 2017년부터 현재까지는 목적 공학과 프롬프트 공학이 주를 이루고 있다. 사전학습된 언어 모델을 사용하여 초기 모델을 설정하고, 목적함수를 엔지니어링하는 것이 이 단계의 특징이다. BERT와 같은 모델이 미세조정을 통해 다양한 작업에 적용되고 있다. 또한, 2019년부터는 프롬프트 공학이 두각을 나타내고 있으며, GPT-3와 같은 모델은 프롬프트를 통해 다양한 NLP 작업을 수행할 수 있다. 이 단계에서는 언어 모델에 대한 의존도가 높아져, 아키텍처나 특징 추출에 대한 부담이 줄어들었다. [@Amatriain2023]

```{mermaid}
graph LR;
    %% Styling
    style A fill:#f9d79c,stroke:#f39c12,stroke-width:2px;
    style B fill:#aed6f1,stroke:#3498db,stroke-width:2px;
    style C fill:#d5f5e3,stroke:#27ae60,stroke-width:2px;
    style D fill:#f5b7b1,stroke:#e74c3c,stroke-width:2px;

    %% Subgraph for Feature Engineering
    subgraph A["피처(Feature) 공학"]
        A1["패러다임: 비신경망 지도학습 <br>전성기: ~2015 <br>특징: 수작업 Feature, 비신경망 사용 <br>대표작: SVM, CRF"]
    end

    %% Subgraph for Architecture Engineering
    subgraph B["아키텍처 공학"]
        B1["패러다임: 신경망 지도학습 <br>전성기: 2013~2018 <br>특징: 신경망 의존, 네트워크 수정 필요 <br>대표작: 텍스트 분류에 CNN"]
    end

    %% Subgraph for Objective Engineering
    subgraph C["목적(Objective) 공학"]
        C1["패러다임: 사전학습, 미세조정 <br>전성기: 2017~현재 <br>특징: 사전학습 LLM 사용, 목적함수 공학 필요 <br>대표작: BERT → Fine Tuning"]
    end

    %% Subgraph for Prompt Engineering
    subgraph D["프롬프트 공학"]
        D1["패러다임: 사전학습, 프롬프트, 예측 <br>전성기: 2019~현재 <br>특징: LLM 전적 의존, 프롬프트 공학 필요 <br>대표작: GPT3"]
    end

    %% Connections
    A --> B
    B --> C
    C --> D
```
